\documentclass[12pt]{article}


\usepackage[dvips,letterpaper,margin=0.75in,bottom=0.5in]{geometry}
\usepackage{cite}
\usepackage{slashed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{braket}
\begin{document}

\title{Statistical Analysis}
\author{Michael Mulhearn}

\maketitle

\section{Likelihood and $\chi^2$}

Suppose we make a series of measurements:
\begin{displaymath}
\{x_1 \pm \sigma_1, x_2 \pm \sigma_2, \ldots, x_n \pm \sigma_n \} \equiv x_i \pm \sigma_i
\end{displaymath}
and we would like to know how likely this outcome is to have occurred as the result of a corresponding theoretical prediction for each measurement:
\begin{displaymath}
\{y_1, y_2, \ldots, y_n \} \equiv y_i
\end{displaymath}
Assuming the uncertainties on each $x$ are Gaussian, the probability of one measurement is:
\begin{displaymath}
P_i = \frac{1}{\sqrt{2 \pi} \sigma_i}  \exp\left( - \frac{(y_i-x_i)^2}{2\sigma_i^2}\right)
\end{displaymath}
And the probability for the complete set of measurements, called the Likelihood, is the product of these probabilities for each measurement:
\begin{displaymath}
{\cal L} = \prod_i P_i = \prod_i \frac{1}{\sqrt{2 \pi} \sigma_i}  \exp\left( - \frac{(y_i-x_i)^2}{2\sigma_i^2}\right)
\end{displaymath}
Now being physicists, we hate products and prefer sums, so we apply a logarithm , and there is an annoying factor of $\frac{1}{2}$ in the exponential, so we multiple by 2, and, generally being pessimists, we prefer to minimize instead of maximize, so we multiple by $-1$, so that at last 
we calculate:
\begin{equation}
-2 \log {\cal L} = \sum_i \frac{(y_i-x_i)^2}{\sigma_i^2} - \log(\sqrt{2\pi}\sigma_i)
\end{equation}
Assuming the experimental uncertainties, $\sigma_i$, are known, the second term is simply a constant.  To maximize the likelihood, we therefore 
minimize:
\begin{equation}
\chi^2 \equiv \sum_i \frac{(y_i-x_i)^2}{\sigma_i^2} 
\end{equation}
A small value of $\chi^2$ means that the result is very close to the theoretical prediction and a large value means that the result is unlikely to have occurred as a result of the prediction.  If the uncertainties and prediction are all correct, we would expect each $x_i$ to differ from the prediction $y_i$ by about $\sigma_i$.  So in this case we would expect:
\begin{equation}
\chi^2 = \sum_i \frac{(y_i-x_i)^2}{\sigma_i^2} \sim \sum_i 1 = N
\end{equation}

\section{Minimizing $\chi^2$}

If we wish to determine the theoretical prediction that best describes our data, we simply minimize the $\chi^2$ (which amounts to maximizing the likelihood).

Consider, for example, the case where we measure a particular quantity $x$ a total of $n$ times, and so obtain measurements $x_i \pm \sigma_i$.  
We wish to extract from our data our best estimate for the true value of $x$.  In this case, the prediction $y_i$ will have the same value for every every $i$, and we construct $\chi^2$ as:
\begin{displaymath}
\chi^2 = \sum_i \frac{(m - x_i)^2}{\sigma_i^2}
\end{displaymath}
Where $m$ is the parameter we wish to extract from the data.  The minimum value of $\chi^2$ occurs at:
\begin{displaymath}
\frac{d\chi^2}{dm} = 0
\end{displaymath}
and so
\begin{eqnarray*}
\frac{d\chi^2}{dm} &=& \sum_i \frac{2(m - x_i)}{\sigma_i^2} = 0\\
0 &=& m \sum_i \frac{1}{\sigma^2_i} - \sum_i \frac{x_i}{\sigma^2_i}\\
m &=& \sum_i w_i x_i
\end{eqnarray*}
where
\begin{equation}
w_i = \frac{1/\sigma^2_i}{\sum_j 1/\sigma_j^2}
\end{equation}
In the case that all the uncertainties are the same $\sigma_i = \sigma$ the weighting factor is simply $w_i = 1 / N$ and we see that the best estimate for $m$ is just the mean value of our measurements.

\section{Extracting Uncertainties}

The example measurement in the preceding section showed us that the best estimate for the true value of $x$ from a series of measurement is a weighted mean.  Since the mean is a sum of measured values, we know how to determine the uncertainty on $m$ by propagating the uncertainty from the individual measurements: 
\begin{eqnarray*}
\sigma^2_m &=& \sum_i \left( \frac{dm}{dx_i} \right)^2 \sigma_i^2\\
\sigma^2_m &=& \sum_i (w_i \sigma_i)^2\\
\sigma^2_m &=& \sum_i \left( \frac{1}{\sum_j 1/\sigma_j^2}\right)^2\\
1/\sigma^2_m &=& \frac{1}{N}\left( \sum_j 1/\sigma_j^2 \right)^2
\end{eqnarray*}
In case the uncertainties are the same, this reduces to:
\begin{displaymath}
\sigma_m^2 = \sigma^2/N
\end{displaymath}

But now let's consider a much more powerful approach.  Let's imagine that we have two parameters $a$ and $b$ that we have determined (using the results of the previous section) to have best fit values $a_0$ and $b_0$.  We would like to determine the uncertainty on these extracted parameters.  The$\chi^2$ sums over the experimental measurements and uncertainties, so in the end it is simply a function of the parameters we seek to determine.   We can therefore think of our experiment as having an equivalent $\chi^2$ where each parameter was simply directly measured with it's (currently unknown)  experimental uncertainty:
\begin{displaymath}
\chi^2 = \frac{(a - a_0)^2}{\sigma_a^2} + \frac{(b - b_0)^2}{\sigma_b^2} 
\end{displaymath}
In this case:
\begin{displaymath}
\frac{1}{2} \frac{d^2 \chi^2}{da^2} = \frac{1}{\sigma_a^2}
\end{displaymath}
So we see that we can extract the uncertainties on our fit parameters from the second derivative of the $\chi^2$.  So the minimum of the $\chi^2$ function is the best fit value, and the curvature at that point is related to the experimental uncertainty on the extracted parameters.

In our example, this leads to:
\begin{displaymath}
\sigma_m^2 = \sigma^2/N
\end{displaymath}

\section{The Best Estimate for $\sigma$}

So far we have presumed that we know the uncertainty associated with each measurement, but suppose we don't know this.

Minimizing $\chi^2$ is no help here, because we can make $\sigma$ as large as we want to minimize $\chi^2$.  This is because we treated the uncertainties as constants!  Return to:
\begin{equation}
-2 \log {\cal L} = \sum_i \left( \frac{(m-x_i)^2}{\sigma^2} - \log(\sqrt{2\pi}\sigma)\right)
\end{equation}

\begin{equation}
-2 \log {\cal L} = \sum_i \frac{(m-x_i)^2}{\sigma^2} - \log(\sqrt{2\pi}\sigma)
\end{equation}

Differenentiating wrt $\sigma$ and setting to zero:

\begin{equation}
0 = \left( \sum_i \frac{(m-x_i)^2}{\sigma^3} \right)-\frac{N}{\sigma} 
\end{equation}

\begin{equation}
\sigma^2 = \frac{\sum_i (m-x_i)^2}{N} 
\end{equation}



\end{document}




